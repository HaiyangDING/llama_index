{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Llama_index\n",
    "\n",
    "Doc [Llama_index with Azure OpenAI](https://docs.llamaindex.ai/en/stable/examples/customization/llms/AzureOpenAI/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from basic examples. Using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print ('Hello')\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "print ('World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ[\"api_key\"]\n",
    "azure_endpoint = os.environ[\"azure_endpoint\"]\n",
    "api_version = os.environ[\"api_version\"]\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "> Source (Doc id: 26c98266-13e2-4abb-9856-22f4f42848f5): Notes\n",
      "\n",
      "[1] My experience skipped a step in the evolution of computers: time-sharing machines wi...\n",
      "\n",
      "> Source (Doc id: c3313b5e-f7d9-4847-8817-914db725fe6b): What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of s...\n",
      "query was: What is most interesting about this essay?\n",
      "answer was: The most interesting aspect of this essay is the author's personal journey and experiences with writing and programming, from his early attempts at writing short stories to his exploration of programming on different computer systems. The essay provides insights into the author's development as a writer and programmer, as well as his changing interests and aspirations throughout his life.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is most interesting about this essay?\"\n",
    "query_engine = index.as_query_engine()\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(answer.get_formatted_sources())\n",
    "print(\"query was:\", query)\n",
    "print(\"answer was:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn 'Query pipeline'\n",
    "\n",
    "[llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline.ipynb](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline.ipynb)\n",
    "\n",
    "Todo:\n",
    "- to store the graph representation. with `networkx` or `pygraphviz`\n",
    "- why do we need a cohere api key?\n",
    "- Finish the walkthrough\n",
    "\n",
    "### Setup\n",
    "\n",
    "Error.\n",
    "\n",
    "```shell\n",
    "Traceback (most recent call last):\n",
    "\n",
    "  File c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553 in run_code\n",
    "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "\n",
    "  Cell In[2], line 2\n",
    "    import phoenix as px\n",
    "\n",
    "  File c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\phoenix\\__init__.py:56\n",
    "    except PhoenixError, e:\n",
    "           ^\n",
    "SyntaxError: multiple exception types must be parenthesized\n",
    "```\n",
    "\n",
    "Try `pip install llama-index-callbacks-arize-phoenix`.\n",
    "\n",
    "Error with install `llama-index-callbacks-arize-phoenix`:\n",
    "\n",
    "```shell\n",
    "      building 'hdbscan._hdbscan_tree' extension\n",
    "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "      [end of output]\n",
    "\n",
    "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
    "  ERROR: Failed building wheel for hdbscan\n",
    "Failed to build hdbscan\n",
    "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n",
    "```\n",
    "\n",
    "Try install MS Visual Studio (with only C++ related options) [ref](https://github.com/run-llama/llama_index/issues/10602#issuecomment-1939692627)\n",
    "\n",
    "This fixes the issue in installing `llama-index-callbacks-arize-phoenix`\n",
    "\n",
    "After `llama-index-callbacks-arize-phoenix` installed, the `import phoenix as px` issue is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:phoenix.datasets.dataset:Dataset: phoenix_dataset_c99fd02a-0f34-4aa7-9401-34c234f5f601 initialized\n",
      "Dataset: phoenix_dataset_c99fd02a-0f34-4aa7-9401-34c234f5f601 initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“º To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "import llama_index.core\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\"./paul_graham\")\n",
    "\n",
    "docs = reader.load_data()\n",
    "\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain together Prompt and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module cf1bf591-492b-4a96-b50f-33b33aee0ca5 with input: \n",
      "movie_name: The Departed\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 252752da-239d-4752-8bb9-829779b1ca55 with input: \n",
      "messages: Please generate related movies to The Departed\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "output = p.run(movie_name=\"The Departed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. Infernal Affairs (2002) - This is the original Hong Kong film that inspired The Departed. It follows a similar storyline of undercover cops infiltrating the criminal underworld.\n",
      "\n",
      "2. Internal Affairs (1990) - This American crime thriller, starring Richard Gere and Andy Garcia, revolves around a corrupt cop and an internal affairs officer determined to expose him.\n",
      "\n",
      "3. The Town (2010) - Directed by and starring Ben Affleck, this crime drama follows a group of bank robbers in Boston who find themselves in a dangerous situation when a heist goes wrong.\n",
      "\n",
      "4. Heat (1995) - Directed by Michael Mann, this crime thriller features Al Pacino and Robert De Niro as a detective and a professional thief, respectively, whose paths cross in a high-stakes game of cat and mouse.\n",
      "\n",
      "5. The Departed (2006) - Although it's the movie that inspired this list, it's worth mentioning The Departed itself. Directed by Martin Scorsese, it tells the story of an undercover cop and a mole in the police force, both trying to identify each other while dealing with the Irish mob in Boston.\n",
      "\n",
      "6. Donnie Brasco (1997) - Based on a true story, this crime drama stars Johnny Depp as an undercover FBI agent who infiltrates the Mafia, forming a close bond with a mobster played by Al Pacino.\n",
      "\n",
      "7. American Gangster (2007) - This Ridley Scott film is based on the true story of Frank Lucas, a Harlem drug lord, and the detective, played by Russell Crowe, who is determined to bring him down.\n",
      "\n",
      "8. The Departed 2 (rumored) - There have been rumors of a potential sequel to The Departed, although no official confirmation or details have been released. Keep an eye out for any future developments.\n",
      "\n",
      "9. Training Day (2001) - Denzel Washington won an Academy Award for his role as a corrupt narcotics detective who takes a rookie cop, played by Ethan Hawke, on a dangerous and morally ambiguous ride-along.\n",
      "\n",
      "10. The Godfather (1972) - Francis Ford Coppola's iconic crime saga follows the Corleone family, led by Marlon Brando as Don Vito Corleone, as they navigate the world of organized crime in New York City.\n"
     ]
    }
   ],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module a03729e8-a136-43af-a01a-e5b00fc00705 with input: \n",
      "movie_name: Toy Story\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 87fad292-b887-4e2b-a8cf-79f9c9c5ff00 with input: \n",
      "messages: Please generate related movies to Toy Story. Output with the following JSON format: \n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{\"$defs\": {\"Movie\": {\"description\": \"Object representing a single movie.\", \"prop...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module bc4a1b9c-e9e9-49d3-8415-4b9ea75ed595 with input: \n",
      "input: assistant: {\n",
      "  \"movies\": [\n",
      "    {\n",
      "      \"name\": \"Finding Nemo\",\n",
      "      \"year\": 2003\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cars\",\n",
      "      \"year\": 2006\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Up\",\n",
      "      \"year\": 2009\n",
      "    },\n",
      "    {...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Object representing a single movie.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"Name of the movie.\")\n",
    "    year: int = Field(..., description=\"Year of the movie.\")\n",
    "\n",
    "\n",
    "class Movies(BaseModel):\n",
    "    \"\"\"Object representing a list of movies.\"\"\"\n",
    "\n",
    "    movies: List[Movie] = Field(..., description=\"List of movies.\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "output_parser = PydanticOutputParser(Movies)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Please generate related movies to {movie_name}. Output with the following JSON format: \n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)\n",
    "\n",
    "# add JSON spec to prompt template\n",
    "json_prompt_tmpl = PromptTemplate(json_prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True)\n",
    "output = p.run(movie_name=\"Toy Story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movies(movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Cars', year=2006), Movie(name='Up', year=2009), Movie(name='Inside Out', year=2015), Movie(name='Coco', year=2017)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 70ba6463-ca9e-4d07-b30c-cb7f53741b7b with input: \n",
      "movie_name: The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 4db51fd4-b8b8-4b53-98e5-ee54aaf3ab18 with input: \n",
      "messages: Please generate related movies to The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module e4c13609-9df9-4487-832b-60d7e49b90ae with input: \n",
      "text: <generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x00000184639E7670>\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 218d70ba-1942-45cc-85d2-b7c1ee9d0ab9 with input: \n",
      "messages: Here's some text:\n",
      "\n",
      "1. Batman Begins (2005)\n",
      "2. The Dark Knight Rises (2012)\n",
      "3. Batman v Superman: Dawn of Justice (2016)\n",
      "4. Man of Steel (2013)\n",
      "5. The Avengers (2012)\n",
      "6. Iron Man (2008)\n",
      "7. Captain Amer...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "1. Batman Begins (2005): A young Bruce Wayne becomes Batman to protect Gotham City from corruption and crime, facing his fears and training under the guidance of Ra's al Ghul.\n",
      "2. The Dark Knight Rises (2012): Batman returns to save Gotham City from the ruthless terrorist Bane, who aims to destroy the city and its symbol of hope, while facing personal sacrifices and challenges.\n",
      "3. Batman v Superman: Dawn of Justice (2016): Batman and Superman clash as their ideologies collide, leading to an epic battle while a new threat emerges, forcing them to unite against a common enemy.\n",
      "4. Man of Steel (2013): The origin story of Superman, as he embraces his powers and faces General Zod, a fellow Kryptonian, who threatens Earth's existence.\n",
      "5. The Avengers (2012): Earth's mightiest heroes, including Iron Man, Captain America, Thor, and Hulk, join forces to stop Loki and his alien army from conquering the world.\n",
      "6. Iron Man (2008): Billionaire Tony Stark becomes Iron Man after being held captive, using his advanced suit to fight against evil forces and protect the innocent.\n",
      "7. Captain America: The Winter Soldier (2014): Captain America teams up with Black Widow and Falcon to uncover a conspiracy within S.H.I.E.L.D., facing a powerful assassin known as the Winter Soldier.\n",
      "8. The Amazing Spider-Man (2012): Peter Parker, as Spider-Man, battles against the Lizard, a monstrous villain created by his father's experiments, while navigating his personal life and love interest.\n",
      "9. Watchmen (2009): Set in an alternate reality, a group of retired vigilantes investigates the murder of one of their own, uncovering a conspiracy that could have catastrophic consequences.\n",
      "10. Sin City (2005): A collection of interconnected stories set in the crime-ridden city of Basin City, featuring corrupt cops, femme fatales, and antiheroes seeking justice.\n",
      "11. V for Vendetta (2005): In a dystopian future, a masked vigilante known as V fights against a totalitarian regime, inspiring the people to rise up against oppression and reclaim their freedom.\n",
      "12. Blade Runner 2049 (2017): A young blade runner uncovers a long-buried secret that leads him to seek out former blade runner Rick Deckard, while unraveling the mysteries surrounding replicants.\n",
      "13. Inception (2010): A skilled thief enters people's dreams to steal information, but is tasked with planting an idea instead, leading to a mind-bending journey through multiple layers of reality.\n",
      "14. The Matrix (1999): A computer hacker discovers the truth about reality, joining a group of rebels fighting against sentient machines that have enslaved humanity in a simulated world.\n",
      "15. The Crow (1994): A musician, resurrected by a supernatural crow, seeks vengeance against the gang responsible for his and his fiancÃ©e's brutal murder, becoming an avenging force."
     ]
    }
   ],
   "source": [
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str2 = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this with a summary of each movie?\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "llm_c = llm.as_query_component(streaming=True)\n",
    "\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True\n",
    ")\n",
    "# p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True)\n",
    "\n",
    "output = p.run(movie_name=\"The Dark Knight\")\n",
    "for o in output:\n",
    "    print(o.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 656d886f-1746-4a28-9d5c-86b3a7766e5f with input: \n",
      "movie_name: Toy Story\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 5e82954c-4839-4925-b419-cacf8bd2e97b with input: \n",
      "messages: Please generate related movies to Toy Story. Output with the following JSON format: \n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{\"$defs\": {\"Movie\": {\"description\": \"Object representing a single movie.\", \"prop...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 44c435dd-a261-4cac-bda4-6bc9ca9e3a9e with input: \n",
      "input: <generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x0000018463AD9250>\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Cars', year=2006), Movie(name='Up', year=2009), Movie(name='Inside Out', year=2015), Movie(name='Coco', year=2017)]\n"
     ]
    }
   ],
   "source": [
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        json_prompt_tmpl,\n",
    "        llm.as_query_component(streaming=True),\n",
    "        output_parser,\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "output = p.run(movie_name=\"Toy Story\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Together Qury Rewrting Workflow (propmt + LLM) with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-postprocessor-cohere-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "# use HyDE to hallucinate answer.\n",
    "prompt_str2 = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module d8b63681-2b37-4cda-94dc-105b0e213a6d with input: \n",
      "topic: college\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module befa5e03-7ac4-47e1-a360-8edb156b59bc with input: \n",
      "messages: Please generate a concise question about Paul Graham's life regarding the following topic college\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 2ed88a25-8b01-4f6a-8b04-b1937ddb6264 with input: \n",
      "query_str: assistant: How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module a052e8d3-3be4-4182-80c1-a5bc1adb84d2 with input: \n",
      "messages: Please write a passage to answer the question\n",
      "Try to include as many key details as possible.\n",
      "\n",
      "\n",
      "How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\n",
      "Passage:\"\"\"\n",
      "\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 13a6c090-5ad4-4c91-8d0d-61634d0a9600 with input: \n",
      "input: assistant: Paul Graham's college experience played a pivotal role in shaping his career and entrepreneurial mindset. As a student at Cornell University, Graham immersed himself in the world of compute...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = p.run(topic=\"college\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Full RAG Pipeline as a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "# TODO COHERE_API_KEY should be set explicitly\n",
    "os.environ[\"COHERE_API_KEY\"]=api_key\n",
    "\n",
    "# define modules\n",
    "prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "reranker = CohereRerank()\n",
    "summarizer = TreeSummarize(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"llm\": llm,\n",
    "        \"prompt_tmpl\": prompt_tmpl,\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "        \"reranker\": reranker,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_keys={'query_str', 'nodes'} optional_keys=set()\n"
     ]
    }
   ],
   "source": [
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n",
    "\n",
    "# look at summarizer input keys\n",
    "print(summarizer.as_query_component().input_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygraphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to fix this part: to store the grapsh representation of the pipeline\n",
    "# using pyvis: encoding error\n",
    "# using pygraphviz: cannot install pygraphviz in the current environment \n",
    "\n",
    "## create graph\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "# net.from_nx(p.dag)\n",
    "# net.show(\"rag_dag.html\")\n",
    "\n",
    "# # another option using `pygraphviz`\n",
    "# from networkx.drawing.nx_agraph import to_agraph\n",
    "# from IPython.display import Image\n",
    "# agraph = to_agraph(p.dag)\n",
    "# agraph.layout(prog=\"dot\")\n",
    "# agraph.draw('rag_dag.png')\n",
    "# display(Image('rag_dag.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl with input: \n",
      "topic: YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: Please generate a question about Paul Graham's life regarding the following topic YC\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module reranker with input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='55ac0e15-649e-46c9-97cc-56b993b30c4d', embedding=None, metadata={'file_path': 'c:\\\\src\\\\HaiyangDING\\\\llama_index\\\\docs\\\\docs\\\\examples\\\\haiyang\\\\paul_graham\\\\paul_gra...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/rerank \"HTTP/1.1 403 Forbidden\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/rerank \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "ename": "ApiError",
     "evalue": "status_code: 403, body: <!doctype html><meta charset=\"utf-8\"><meta name=viewport content=\"width=device-width, initial-scale=1\"><title>403</title>403 Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\cohere\\base_client.py:1213\u001b[0m, in \u001b[0;36mBaseCohere.rerank\u001b[1;34m(self, model, query, documents, top_n, return_documents, max_chunks_per_doc, request_options)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     _response_json \u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\httpx\\_models.py:764\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Needs cohere api key.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrun(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:319\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     query_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    317\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_payload}\n\u001b[0;32m    318\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;241m*\u001b[39margs, return_values_direct\u001b[38;5;241m=\u001b[39mreturn_values_direct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    321\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:442\u001b[0m, in \u001b[0;36mQueryPipeline._run\u001b[1;34m(self, return_values_direct, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m root_key, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_root_key_and_kwargs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m# call run_multi with one root key\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m result_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_multi({root_key: kwargs})\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_result_output(result_outputs, return_values_direct)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:544\u001b[0m, in \u001b[0;36mQueryPipeline._run_multi\u001b[1;34m(self, module_input_dict)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    543\u001b[0m     print_debug_input(module_key, module_input)\n\u001b[1;32m--> 544\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mrun_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_input)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# get new nodes and is_leaf\u001b[39;00m\n\u001b[0;32m    547\u001b[0m queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_component_output(\n\u001b[0;32m    548\u001b[0m     queue, output_dict, module_key, all_module_inputs, result_outputs\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:199\u001b[0m, in \u001b[0;36mQueryComponent.run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_dict)\n\u001b[0;32m    198\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_inputs(kwargs)\n\u001b[1;32m--> 199\u001b[0m component_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_outputs(component_outputs)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:102\u001b[0m, in \u001b[0;36mPostprocessorComponent._run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_component\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run component.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessor\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[0;32m    103\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m], query_str\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_str\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: output}\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:55\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[1;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_nodes(nodes, query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\postprocessor\\cohere_rerank\\base.py:63\u001b[0m, in \u001b[0;36mCohereRerank._postprocess_nodes\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mRERANKING,\n\u001b[0;32m     55\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     },\n\u001b[0;32m     61\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m     62\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [node\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content() \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[1;32m---> 63\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrerank(\n\u001b[0;32m     64\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     65\u001b[0m         top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n,\n\u001b[0;32m     66\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m     67\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mresults:\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\cohere\\base_client.py:1215\u001b[0m, in \u001b[0;36mBaseCohere.rerank\u001b[1;34m(self, model, query, documents, top_n, return_documents, max_chunks_per_doc, request_options)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     _response_json \u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[1;32m-> 1215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[1;31mApiError\u001b[0m: status_code: 403, body: <!doctype html><meta charset=\"utf-8\"><meta name=viewport content=\"width=device-width, initial-scale=1\"><title>403</title>403 Forbidden"
     ]
    }
   ],
   "source": [
    "# TODO: Needs cohere api key.\n",
    "# TODO: fix the issue with the graph representation\n",
    "\n",
    "# response = p.run(topic=\"YC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
