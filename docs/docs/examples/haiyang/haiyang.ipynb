{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Llama_index\n",
    "\n",
    "Doc [Llama_index with Azure OpenAI](https://docs.llamaindex.ai/en/stable/examples/customization/llms/AzureOpenAI/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from basic examples. Using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print ('Hello')\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "print ('World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ[\"api_key\"]\n",
    "azure_endpoint = os.environ[\"azure_endpoint\"]\n",
    "api_version = os.environ[\"api_version\"]\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "> Source (Doc id: 74b0de7e-ccf5-4e8e-beee-4de897408c91): Notes\n",
      "\n",
      "[1] My experience skipped a step in the evolution of computers: time-sharing machines wi...\n",
      "\n",
      "> Source (Doc id: 468b3560-e80e-4102-ae67-632389d42671): What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of s...\n",
      "query was: What is most interesting about this essay?\n",
      "answer was: The most interesting aspect of this essay is the author's personal journey and experiences with writing and programming, from his early attempts at writing short stories to his exploration of programming on different computer systems. The essay provides insights into the author's development as a writer and programmer, as well as his evolving interests and aspirations.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is most interesting about this essay?\"\n",
    "query_engine = index.as_query_engine()\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(answer.get_formatted_sources())\n",
    "print(\"query was:\", query)\n",
    "print(\"answer was:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn 'Query pipeline'\n",
    "\n",
    "[llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline.ipynb](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/pipeline/query_pipeline.ipynb)\n",
    "\n",
    "Todo:\n",
    "- to store the graph representation. with `networkx` or `pygraphviz`\n",
    "- why do we need a cohere api key?\n",
    "- Finish the walkthrough\n",
    "\n",
    "### Setup\n",
    "\n",
    "Error.\n",
    "\n",
    "```shell\n",
    "Traceback (most recent call last):\n",
    "\n",
    "  File c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553 in run_code\n",
    "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "\n",
    "  Cell In[2], line 2\n",
    "    import phoenix as px\n",
    "\n",
    "  File c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\phoenix\\__init__.py:56\n",
    "    except PhoenixError, e:\n",
    "           ^\n",
    "SyntaxError: multiple exception types must be parenthesized\n",
    "```\n",
    "\n",
    "Try `pip install llama-index-callbacks-arize-phoenix`.\n",
    "\n",
    "Error with install `llama-index-callbacks-arize-phoenix`:\n",
    "\n",
    "```shell\n",
    "      building 'hdbscan._hdbscan_tree' extension\n",
    "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "      [end of output]\n",
    "\n",
    "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
    "  ERROR: Failed building wheel for hdbscan\n",
    "Failed to build hdbscan\n",
    "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n",
    "```\n",
    "\n",
    "Try install MS Visual Studio (with only C++ related options) [ref](https://github.com/run-llama/llama_index/issues/10602#issuecomment-1939692627)\n",
    "\n",
    "This fixes the issue in installing `llama-index-callbacks-arize-phoenix`\n",
    "\n",
    "After `llama-index-callbacks-arize-phoenix` installed, the `import phoenix as px` issue is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "import llama_index.core\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"embedding-ada-002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\"./paul_graham\")\n",
    "\n",
    "docs = reader.load_data()\n",
    "\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain together Prompt and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 02de9f7b-551e-4964-82a9-a2da152a49d2 with input: \n",
      "movie_name: The Departed\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 9f252eda-f354-4a13-9c95-c6ce4a39421f with input: \n",
      "messages: Please generate related movies to The Departed\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "output = p.run(movie_name=\"The Departed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. Infernal Affairs (2002) - This Hong Kong crime thriller is the original film that inspired The Departed. It follows a similar storyline of undercover cops infiltrating a criminal organization.\n",
      "\n",
      "2. Internal Affairs (1990) - This American crime thriller starring Richard Gere and Andy Garcia revolves around a corrupt cop and an internal affairs officer determined to expose him.\n",
      "\n",
      "3. The Town (2010) - Directed by and starring Ben Affleck, this crime drama follows a group of bank robbers in Boston who find themselves in a dangerous situation when they take a hostage during a heist.\n",
      "\n",
      "4. Heat (1995) - Directed by Michael Mann, this crime thriller features Al Pacino and Robert De Niro as a detective and a professional thief, respectively, whose paths cross in a high-stakes game of cat and mouse.\n",
      "\n",
      "5. The Departed (2006) - Although it's the same movie as the one mentioned in the prompt, it's worth noting that The Departed itself is a highly acclaimed crime drama directed by Martin Scorsese, starring Leonardo DiCaprio, Matt Damon, and Jack Nicholson. It follows the intertwining lives of an undercover cop and a mole in the police force.\n",
      "\n",
      "6. Donnie Brasco (1997) - Based on a true story, this crime drama stars Johnny Depp as an undercover FBI agent who infiltrates the mob and forms a close bond with a low-level mobster played by Al Pacino.\n",
      "\n",
      "7. American Gangster (2007) - This crime film, directed by Ridley Scott, is based on the true story of Frank Lucas, a drug lord in Harlem during the 1970s, and the detective who is determined to bring him down, played by Russell Crowe.\n",
      "\n",
      "8. Training Day (2001) - Denzel Washington won an Academy Award for his portrayal of a corrupt narcotics detective who takes a rookie cop, played by Ethan Hawke, on a day-long journey through the dangerous streets of Los Angeles.\n",
      "\n",
      "9. The Godfather (1972) - Francis Ford Coppola's iconic crime film follows the Corleone crime family and their rise to power in the world of organized crime. It stars Marlon Brando, Al Pacino, and James Caan.\n",
      "\n",
      "10. The Untouchables (1987) - Directed by Brian De Palma, this crime drama tells the story of Eliot Ness, a federal agent who forms a team to take down the notorious gangster Al Capone during the Prohibition era. It stars Kevin Costner, Sean Connery, and Robert De Niro.\n"
     ]
    }
   ],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 540e275e-717d-4122-8374-ecaed4f413da with input: \n",
      "movie_name: Toy Story\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 69db0f3a-5cec-45fa-b2c3-2f37bf029027 with input: \n",
      "messages: Please generate related movies to Toy Story. Output with the following JSON format: \n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{\"$defs\": {\"Movie\": {\"description\": \"Object representing a single movie.\", \"prop...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 6fae8fb9-61b9-4b76-93c6-4dd98cfc48e2 with input: \n",
      "input: assistant: {\n",
      "  \"movies\": [\n",
      "    {\n",
      "      \"name\": \"Finding Nemo\",\n",
      "      \"year\": 2003\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Monsters, Inc.\",\n",
      "      \"year\": 2001\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"The Incredibles\",\n",
      "      \"y...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Object representing a single movie.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"Name of the movie.\")\n",
    "    year: int = Field(..., description=\"Year of the movie.\")\n",
    "\n",
    "\n",
    "class Movies(BaseModel):\n",
    "    \"\"\"Object representing a list of movies.\"\"\"\n",
    "\n",
    "    movies: List[Movie] = Field(..., description=\"List of movies.\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "output_parser = PydanticOutputParser(Movies)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Please generate related movies to {movie_name}. Output with the following JSON format: \n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)\n",
    "\n",
    "# add JSON spec to prompt template\n",
    "json_prompt_tmpl = PromptTemplate(json_prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True)\n",
    "output = p.run(movie_name=\"Toy Story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movies(movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Monsters, Inc.', year=2001), Movie(name='The Incredibles', year=2004), Movie(name='Cars', year=2006), Movie(name='Ratatouille', year=2007), Movie(name='WALL-E', year=2008), Movie(name='Up', year=2009), Movie(name='Toy Story 2', year=1999), Movie(name='Toy Story 3', year=2010), Movie(name='Toy Story 4', year=2019)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 7766d971-6039-467b-9b13-1b3c170434bc with input: \n",
      "movie_name: The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module fdeb2090-cbf3-41ed-a1a8-0cc435e401da with input: \n",
      "messages: Please generate related movies to The Dark Knight\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module cd0af534-9c9f-4b2f-9d4b-cf2da4c29c40 with input: \n",
      "text: <generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x000002B72F3C9BE0>\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 7b48a3b1-8244-4275-8222-4753220cfd35 with input: \n",
      "messages: Here's some text:\n",
      "\n",
      "1. Batman Begins (2005)\n",
      "2. The Dark Knight Rises (2012)\n",
      "3. Batman v Superman: Dawn of Justice (2016)\n",
      "4. Man of Steel (2013)\n",
      "5. The Avengers (2012)\n",
      "6. Iron Man (2008)\n",
      "7. Captain Amer...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "1. Batman Begins (2005): A young Bruce Wayne becomes Batman to protect Gotham City from corruption and crime, facing his fears and training under the guidance of Ra's al Ghul.\n",
      "2. The Dark Knight Rises (2012): Batman returns to save Gotham City from the ruthless terrorist Bane, who aims to destroy the city and its symbol of hope.\n",
      "3. Batman v Superman: Dawn of Justice (2016): Batman and Superman clash as their ideologies collide, leading to an epic battle while a new threat, Lex Luthor, manipulates them both.\n",
      "4. Man of Steel (2013): The origin story of Superman, as he embraces his powers and faces General Zod, a fellow Kryptonian seeking to destroy Earth.\n",
      "5. The Avengers (2012): Earth's mightiest heroes, including Iron Man, Captain America, Thor, and Hulk, unite to stop Loki and his alien army from conquering the world.\n",
      "6. Iron Man (2008): Billionaire Tony Stark builds a high-tech suit to escape captivity and becomes Iron Man, using his technology to fight against evil forces.\n",
      "7. Captain America: The Winter Soldier (2014): Captain America teams up with Black Widow and Falcon to uncover a conspiracy within S.H.I.E.L.D. while facing a deadly assassin known as the Winter Soldier.\n",
      "8. The Amazing Spider-Man (2012): Peter Parker, a high school student bitten by a radioactive spider, becomes Spider-Man and battles the Lizard, a monstrous villain threatening New York City.\n",
      "9. Watchmen (2009): Set in an alternate reality, a group of retired vigilantes investigates the murder of one of their own, uncovering a conspiracy that could change the world.\n",
      "10. Sin City (2005): A collection of interconnected stories set in the crime-ridden Basin City, featuring corrupt cops, femme fatales, and hardened criminals.\n",
      "11. V for Vendetta (2005): In a dystopian future, a masked vigilante known as V fights against a totalitarian regime, inspiring the people to rise up against oppression.\n",
      "12. Blade Runner 2049 (2017): A young blade runner uncovers a long-buried secret that leads him to seek out former blade runner Rick Deckard, while unraveling the mysteries of a future society.\n",
      "13. Inception (2010): A skilled thief enters people's dreams to steal information, but is tasked with planting an idea instead, leading to a mind-bending journey through multiple layers of reality.\n",
      "14. The Matrix (1999): A computer hacker discovers the truth about reality, joining a group of rebels fighting against sentient machines that have enslaved humanity in a simulated world.\n",
      "15. The Crow (1994): A musician, resurrected by a supernatural crow, seeks revenge against the gang who murdered him and his fianc√©e, unleashing his dark and vengeful powers."
     ]
    }
   ],
   "source": [
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "# let's add some subsequent prompts for fun\n",
    "prompt_str2 = \"\"\"\\\n",
    "Here's some text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Can you rewrite this with a summary of each movie?\n",
    "\"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "llm_c = llm.as_query_component(streaming=True)\n",
    "\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl, llm_c, prompt_tmpl2, llm_c], verbose=True\n",
    ")\n",
    "# p = QueryPipeline(chain=[prompt_tmpl, llm_c], verbose=True)\n",
    "\n",
    "output = p.run(movie_name=\"The Dark Knight\")\n",
    "for o in output:\n",
    "    print(o.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 84bd3d3a-3ada-4234-9029-d8ea29e09ef1 with input: \n",
      "movie_name: Toy Story\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module cd6b792b-7beb-4022-8e96-cf71999c21e5 with input: \n",
      "messages: Please generate related movies to Toy Story. Output with the following JSON format: \n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{\"$defs\": {\"Movie\": {\"description\": \"Object representing a single movie.\", \"prop...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 275c948a-cf0b-44d0-9bf2-873d4fc5621f with input: \n",
      "input: <generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x000002B72F3CA020>\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Cars', year=2006), Movie(name='Up', year=2009), Movie(name='Inside Out', year=2015), Movie(name='Coco', year=2017)]\n"
     ]
    }
   ],
   "source": [
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        json_prompt_tmpl,\n",
    "        llm.as_query_component(streaming=True),\n",
    "        output_parser,\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "output = p.run(movie_name=\"Toy Story\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Together Qury Rewrting Workflow (propmt + LLM) with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-postprocessor-cohere-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Please generate a concise question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "# use HyDE to hallucinate answer.\n",
    "prompt_str2 = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "p = QueryPipeline(\n",
    "    chain=[prompt_tmpl1, llm, prompt_tmpl2, llm, retriever], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module a3107822-baee-41d7-950f-fcc961bd235b with input: \n",
      "topic: college\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 7099edc2-a365-4b9a-861e-b1af892d26f6 with input: \n",
      "messages: Please generate a concise question about Paul Graham's life regarding the following topic college\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 5d7e437b-1aab-425a-9338-7a54722b3ba0 with input: \n",
      "query_str: assistant: How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 77b2062f-a9b0-4a10-b27a-8e063b556c74 with input: \n",
      "messages: Please write a passage to answer the question\n",
      "Try to include as many key details as possible.\n",
      "\n",
      "\n",
      "How did Paul Graham's college experience shape his career and entrepreneurial mindset?\n",
      "\n",
      "\n",
      "Passage:\"\"\"\n",
      "\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module 19c319c8-a46e-4f4e-8729-39e6e4738c19 with input: \n",
      "input: assistant: Paul Graham's college experience played a pivotal role in shaping his career and entrepreneurial mindset. As a student at Cornell University, Graham immersed himself in the world of compute...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = p.run(topic=\"college\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Full RAG Pipeline as a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "# TODO COHERE_API_KEY should be set explicitly\n",
    "os.environ[\"COHERE_API_KEY\"]=api_key\n",
    "\n",
    "# define modules\n",
    "prompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    deployment_name=\"gpt35\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "reranker = CohereRerank()\n",
    "summarizer = TreeSummarize(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"llm\": llm,\n",
    "        \"prompt_tmpl\": prompt_tmpl,\n",
    "        \"retriever\": retriever,\n",
    "        \"summarizer\": summarizer,\n",
    "        \"reranker\": reranker,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_keys={'nodes', 'query_str'} optional_keys=set()\n"
     ]
    }
   ],
   "source": [
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "p.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n",
    "\n",
    "# look at summarizer input keys\n",
    "print(summarizer.as_query_component().input_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvis\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (8.23.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (3.1.3)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis)\n",
      "  Downloading jsonpickle-3.0.3-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyvis) (3.2.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.14.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\haiyang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
      "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "   ---------------------------------------- 0.0/756.0 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/756.0 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/756.0 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/756.0 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 30.7/756.0 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 30.7/756.0 kB 262.6 kB/s eta 0:00:03\n",
      "   -- ------------------------------------ 41.0/756.0 kB 163.4 kB/s eta 0:00:05\n",
      "   ---- ---------------------------------- 81.9/756.0 kB 269.5 kB/s eta 0:00:03\n",
      "   ---- ---------------------------------- 92.2/756.0 kB 275.8 kB/s eta 0:00:03\n",
      "   ----- -------------------------------- 112.6/756.0 kB 297.7 kB/s eta 0:00:03\n",
      "   ------- ------------------------------ 143.4/756.0 kB 327.9 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 163.8/756.0 kB 350.7 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 194.6/756.0 kB 393.0 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 245.8/756.0 kB 430.6 kB/s eta 0:00:02\n",
      "   -------------- ----------------------- 286.7/756.0 kB 478.3 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 327.7/756.0 kB 507.9 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 399.4/756.0 kB 565.6 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 481.3/756.0 kB 641.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 563.2/756.0 kB 693.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 624.6/756.0 kB 742.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 727.0/756.0 kB 804.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 756.0/756.0 kB 823.2 kB/s eta 0:00:00\n",
      "Downloading jsonpickle-3.0.3-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.8/40.8 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-3.0.3 pyvis-0.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygraphviz\n",
      "  Downloading pygraphviz-1.12.tar.gz (104 kB)\n",
      "     ---------------------------------------- 0.0/104.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/104.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/104.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/104.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/104.9 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/104.9 kB 330.3 kB/s eta 0:00:01\n",
      "     ---------- -------------------------- 30.7/104.9 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------- ---------------------- 41.0/104.9 kB 196.9 kB/s eta 0:00:01\n",
      "     --------------------- --------------- 61.4/104.9 kB 234.9 kB/s eta 0:00:01\n",
      "     -------------------------------- ---- 92.2/104.9 kB 291.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ---- 92.2/104.9 kB 291.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 104.9/104.9 kB 276.1 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pygraphviz\n",
      "  Building wheel for pygraphviz (pyproject.toml): started\n",
      "  Building wheel for pygraphviz (pyproject.toml): finished with status 'error'\n",
      "Failed to build pygraphviz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for pygraphviz (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [55 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\agraph.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\graphviz.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\scraper.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\testing.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\__init__.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      creating build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_attribute_defaults.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_clear.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_close.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_drawing.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_edge_attributes.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_graph.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_html.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_layout.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_node_attributes.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_readwrite.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_repr_mimebundle.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_scraper.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_string.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_subgraph.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_unicode.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\pygraphviz\\tests\n",
      "      running egg_info\n",
      "      writing pygraphviz.egg-info\\PKG-INFO\n",
      "      writing dependency_links to pygraphviz.egg-info\\dependency_links.txt\n",
      "      writing top-level names to pygraphviz.egg-info\\top_level.txt\n",
      "      reading manifest file 'pygraphviz.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching '*.png' under directory 'doc'\n",
      "      warning: no files found matching '*.html' under directory 'doc'\n",
      "      warning: no files found matching '*.txt' under directory 'doc'\n",
      "      warning: no files found matching '*.css' under directory 'doc'\n",
      "      warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "      warning: no previously-included files matching '.svn' found anywhere in distribution\n",
      "      no previously-included directories found matching 'doc\\build'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'pygraphviz.egg-info\\SOURCES.txt'\n",
      "      copying pygraphviz\\graphviz.i -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      copying pygraphviz\\graphviz_wrap.c -> build\\lib.win-amd64-cpython-312\\pygraphviz\n",
      "      running build_ext\n",
      "      building 'pygraphviz._graphviz' extension\n",
      "      creating build\\temp.win-amd64-cpython-312\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\pygraphviz\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -DSWIG_PYTHON_STRICT_BYTE_CHAR -DGVDLL -IC:\\Users\\haiyang\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\haiyang\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /Tcpygraphviz/graphviz_wrap.c /Fobuild\\temp.win-amd64-cpython-312\\Release\\pygraphviz/graphviz_wrap.obj\n",
      "      graphviz_wrap.c\n",
      "      pygraphviz/graphviz_wrap.c(3020): fatal error C1083: Êó†Ê≥ïÊâìÂºÄÂåÖÊã¨Êñá‰ª∂: ‚Äúgraphviz/cgraph.h‚Äù: No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pygraphviz\n",
      "ERROR: Could not build wheels for pygraphviz, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "# !pip install pygraphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_dag.html\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'gbk' codec can't encode character '\\xa9' in position 230710: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m net \u001b[38;5;241m=\u001b[39m Network(notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cdn_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_line\u001b[39m\u001b[38;5;124m\"\u001b[39m, directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m net\u001b[38;5;241m.\u001b[39mfrom_nx(p\u001b[38;5;241m.\u001b[39mdag)\n\u001b[1;32m----> 6\u001b[0m net\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_dag.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\pyvis\\network.py:546\u001b[0m, in \u001b[0;36mNetwork.show\u001b[1;34m(self, name, local, notebook)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebook:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_html(name, open_browser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,notebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_html(name, open_browser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\pyvis\\network.py:530\u001b[0m, in \u001b[0;36mNetwork.write_html\u001b[1;34m(self, name, local, notebook, open_browser)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdn_resources \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_line\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdn_resources \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremote\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(getcwd_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m--> 530\u001b[0m         out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdn_resources is not in [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_line\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremote\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'gbk' codec can't encode character '\\xa9' in position 230710: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# TODO: Need to fix this part: to store the grapsh representation of the pipeline\n",
    "\n",
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(p.dag)\n",
    "net.show(\"rag_dag.html\")\n",
    "\n",
    "# # another option using `pygraphviz`\n",
    "# from networkx.drawing.nx_agraph import to_agraph\n",
    "# from IPython.display import Image\n",
    "# agraph = to_agraph(p.dag)\n",
    "# agraph.layout(prog=\"dot\")\n",
    "# agraph.draw('rag_dag.png')\n",
    "# display(Image('rag_dag.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl with input: \n",
      "topic: YC\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: Please generate a question about Paul Graham's life regarding the following topic YC\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/gpt35/chat/completions?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://haiyang-azopenai-test.openai.azure.com//openai/deployments/embedding-ada-002/embeddings?api-version=2023-07-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module reranker with input: \n",
      "query_str: assistant: What role did Paul Graham play in the founding and development of Y Combinator (YC)?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='55ac0e15-649e-46c9-97cc-56b993b30c4d', embedding=None, metadata={'file_path': 'c:\\\\src\\\\HaiyangDING\\\\llama_index\\\\docs\\\\docs\\\\examples\\\\haiyang\\\\paul_graham\\\\paul_gra...\n",
      "\n",
      "\u001b[0mINFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/rerank \"HTTP/1.1 401 Unauthorized\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/rerank \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "ApiError",
     "evalue": "status_code: 401, body: {'message': 'invalid api token'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrun(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:319\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     query_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    317\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_payload}\n\u001b[0;32m    318\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;241m*\u001b[39margs, return_values_direct\u001b[38;5;241m=\u001b[39mreturn_values_direct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    321\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:442\u001b[0m, in \u001b[0;36mQueryPipeline._run\u001b[1;34m(self, return_values_direct, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m root_key, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_root_key_and_kwargs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m# call run_multi with one root key\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m result_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_multi({root_key: kwargs})\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_result_output(result_outputs, return_values_direct)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:544\u001b[0m, in \u001b[0;36mQueryPipeline._run_multi\u001b[1;34m(self, module_input_dict)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    543\u001b[0m     print_debug_input(module_key, module_input)\n\u001b[1;32m--> 544\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mrun_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_input)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# get new nodes and is_leaf\u001b[39;00m\n\u001b[0;32m    547\u001b[0m queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_component_output(\n\u001b[0;32m    548\u001b[0m     queue, output_dict, module_key, all_module_inputs, result_outputs\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:199\u001b[0m, in \u001b[0;36mQueryComponent.run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_dict)\n\u001b[0;32m    198\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_inputs(kwargs)\n\u001b[1;32m--> 199\u001b[0m component_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_outputs(component_outputs)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:102\u001b[0m, in \u001b[0;36mPostprocessorComponent._run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_component\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run component.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessor\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[0;32m    103\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m], query_str\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_str\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: output}\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:55\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[1;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_nodes(nodes, query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\postprocessor\\cohere_rerank\\base.py:63\u001b[0m, in \u001b[0;36mCohereRerank._postprocess_nodes\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mRERANKING,\n\u001b[0;32m     55\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     },\n\u001b[0;32m     61\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m     62\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [node\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content() \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[1;32m---> 63\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrerank(\n\u001b[0;32m     64\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     65\u001b[0m         top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n,\n\u001b[0;32m     66\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m     67\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mresults:\n",
      "File \u001b[1;32mc:\\Users\\haiyang\\miniconda3\\envs\\llamaindex\\Lib\\site-packages\\cohere\\base_client.py:1216\u001b[0m, in \u001b[0;36mBaseCohere.rerank\u001b[1;34m(self, model, query, documents, top_n, return_documents, max_chunks_per_doc, request_options)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[1;31mApiError\u001b[0m: status_code: 401, body: {'message': 'invalid api token'}"
     ]
    }
   ],
   "source": [
    "response = p.run(topic=\"YC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
